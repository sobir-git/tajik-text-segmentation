from src.tokenizer import tokenize_text


def test_tokenize_text():
    text = '''–°–∞–ª–æ–º, “∑–∞“≥–æ–Ω!'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–°–∞–ª–æ–º,', ' “∑–∞“≥–æ–Ω!']
    assert spans == [(0,6), (6,13)]
    
    text = '''\n–ì—É—Ä–±–∞–∏üêà\t–±–µ–º–µ“≥—Ä.'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['\n–ì—É—Ä–±–∞–∏üêà', '\t–±–µ–º–µ“≥—Ä.']

    text = '''–°–∞–ª–æ–º, “∑–∞“≥–æ–Ω!\n–ì—É—Ä–±–∞–∏üêà\t–±–µ–º–µ“≥—Ä.'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–°–∞–ª–æ–º,', ' “∑–∞“≥–æ–Ω!', '\n–ì—É—Ä–±–∞–∏üêà', '\t–±–µ–º–µ“≥—Ä.']

    text = '''–∑–∏–Ω–¥–∞ –º–µ–∫—É–Ω–∞–Ω–¥,\n –ê–∑ –º—É“≥–∞–±–±–∞—Ç'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–∑–∏–Ω–¥–∞', ' –º–µ–∫—É–Ω–∞–Ω–¥,', '\n –ê–∑', ' –º—É“≥–∞–±–±–∞—Ç']

    text = '''–¥–∞—Ä –∫—É“∑–æ—Å—Ç? ‚Äì –ø—É—Ä—Å–∏–¥'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–¥–∞—Ä', ' –∫—É“∑–æ—Å—Ç?', ' ‚Äì –ø—É—Ä—Å–∏–¥']

    text = '''–ú—É“≥–∞–º–º–∞–¥—ä–∞–ª”£ ( 1347);'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–ú—É“≥–∞–º–º–∞–¥—ä–∞–ª”£', ' ( 1347);']

    text = '''–º–µ–ø—É—Ä—Å–∞–º, –∫–∏: ¬ª -¬´–û–±–∏–≤—É'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–º–µ–ø—É—Ä—Å–∞–º,', ' –∫–∏: ¬ª', ' -¬´–û–±–∏–≤—É']

    text = '''—Ñ–∞—Ä–æ“≥–∞–º —à—É–¥.\n–§–∞—Ä“≥–∞–Ω–≥—Å–∞—Ä–æ–∏ –æ—Ä–∏—ë”£'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['—Ñ–∞—Ä–æ“≥–∞–º', ' —à—É–¥.', '\n–§–∞—Ä“≥–∞–Ω–≥—Å–∞—Ä–æ–∏', ' –æ—Ä–∏—ë”£']

    text = '''—è“≥—É–¥–æ–Ω).\n3. –û–Ω“≥–æ–µ–∞–Ω–¥, –∫–∏'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['—è“≥—É–¥–æ–Ω).', '\n3.', ' –û–Ω“≥–æ–µ–∞–Ω–¥,', ' –∫–∏']

    text = '''–ë–∞—Ä–æ–∏ —á”£ ?" —Å—É–æ–ª–∏ –∫–æ—Ä–∏'''
    tokens, spans = tokenize_text(text)
    assert tokens == ['–ë–∞—Ä–æ–∏', ' —á”£ ?', '"', ' —Å—É–æ–ª–∏', ' –∫–æ—Ä–∏']

